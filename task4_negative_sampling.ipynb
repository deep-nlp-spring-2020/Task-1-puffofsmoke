{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "task4_negative_sampling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eb2BK4_AB9mv"
      },
      "source": [
        "# Assignment 1.4: Negative sampling (15 points)\n",
        "\n",
        "You may have noticed that word2vec is really slow to train. Especially with big (> 50 000) vocabularies. Negative sampling is the solution.\n",
        "\n",
        "The task is to implement word2vec with negative sampling.\n",
        "\n",
        "This is what was discussed in Stanford lecture. The main idea is in the formula:\n",
        "\n",
        "$$ L = \\log\\sigma(u^T_o \\cdot u_c) + \\sum^k_{i=1} \\mathbb{E}_{j \\sim P(w)}[\\log\\sigma(-u^T_j \\cdot u_c)]$$\n",
        "\n",
        "Where $\\sigma$ - sigmoid function, $u_c$ - central word vector, $u_o$ - context (outside of the window) word vector, $u_j$ - vector or word with index $j$.\n",
        "\n",
        "The first term calculates the similarity between positive examples (word from one window)\n",
        "\n",
        "The second term is responsible for negative samples. $k$ is a hyperparameter - the number of negatives to sample.\n",
        "$\\mathbb{E}_{j \\sim P(w)}$\n",
        "means that $j$ is distributed accordingly to unigram distribution.\n",
        "\n",
        "Thus, it is only required to calculate the similarity between positive samples and some other negatives. Not across all the vocabulary.\n",
        "\n",
        "Useful links:\n",
        "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dMIyJuRYYzAW",
        "colab_type": "text"
      },
      "source": [
        "# Implementation\n",
        "## Use Google Colab with GPU to reproduce result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFSkFZFtZTos",
        "colab_type": "text"
      },
      "source": [
        "## Upload text8.zip please before run next cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zqaFdK03YzAw",
        "colab_type": "code",
        "outputId": "48695e52-ad6f-4b5f-a413-aa9a21e8894b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!rm ./text8\n",
        "!unzip ./text8.zip"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./text8.zip\n",
            "  inflating: text8                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lC9SIcrZXoT",
        "colab_type": "text"
      },
      "source": [
        "## All imports here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3JYWtIqGYzBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from tsnecuda import TSNE\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6n6YEheZb5p",
        "colab_type": "text"
      },
      "source": [
        "## Batcher for Skip-Gram or CBOW from previous assigment\n",
        "- torch.tensor instead of np.array used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IguTMv7yYzBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batcher():\n",
        "    def __init__(self, text, limit=5, window_size=2, mode='sg'):\n",
        "        \"\"\"\n",
        "        Batcher for Skip-Gram or CBOW\n",
        "\n",
        "        :param text: String without newline symbols\n",
        "        :param limit: Don't put words with less amount into the dictonary\n",
        "        :param window_size: Window size ^)\n",
        "        :param mode: cbow or sg\n",
        "        \"\"\"\n",
        "\n",
        "        self.limit = limit\n",
        "        self.window_size = window_size\n",
        "        self.mode = mode\n",
        "\n",
        "        self.text = text\n",
        "\n",
        "        self.UNK = 'UNK'\n",
        "\n",
        "        self.tokens = []\n",
        "        self.tokens_ind = []\n",
        "\n",
        "\n",
        "        self.vocabulary = set()\n",
        "        self.word2index = dict()\n",
        "        self.index2word = []\n",
        "\n",
        "        self.most_common = []\n",
        "\n",
        "        self._preprocess()\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, path, limit=5, window_size=2, mode='sg'):\n",
        "        \"\"\"\n",
        "        Init Batcher from file\n",
        "\n",
        "        :param path: Path to text file\n",
        "        :param limit: Don't put words with less amount into the dictonary\n",
        "        :param window_size: Window size ^)\n",
        "        :param mode: cbow or sg\n",
        "        :return: Batcher object\n",
        "        \"\"\"\n",
        "        n = -1\n",
        "        with open(path) as f:\n",
        "            text = f.read(n)\n",
        "\n",
        "        return cls(text, limit, window_size, mode)\n",
        "\n",
        "    def _clean(self):\n",
        "        # Everething expect [a-z ] already killed in our dataset\n",
        "        pass\n",
        "\n",
        "    def _tokenize(self):\n",
        "        # Stupid tokenizer for our dataset\n",
        "        self.tokens = self.text.split()\n",
        "\n",
        "    def _build_vocabulary(self):\n",
        "        counter_words = Counter(self.tokens)\n",
        "        self.counter_words = counter_words\n",
        "        self.vocabulary = {word for word, counts in counter_words.items() if counts >= self.limit}\n",
        "        most_common = counter_words.most_common(200)\n",
        "        self.most_common = list(zip(*most_common))[0]\n",
        "\n",
        "    def _numericalize(self):\n",
        "        self.index2word = [self.UNK] + list(self.vocabulary)\n",
        "        self.vocabulary.add(self.UNK)\n",
        "        self.word2index = dict(zip(self.index2word, range(len(self.index2word))))\n",
        "        self.tokens_ind = [self.word2index.get(word, 0) for word in self.tokens]\n",
        "\n",
        "    def _preprocess(self):\n",
        "        self._clean()\n",
        "        self._tokenize()\n",
        "        self._build_vocabulary()\n",
        "        self._numericalize()\n",
        "\n",
        "    def indices2words(self, indices):\n",
        "        shape = indices.shape\n",
        "        result = np.array([self.index2word[idx] for idx in indices.flatten()])\n",
        "        return result.reshape(shape)\n",
        "\n",
        "    def batch_generator(self, batch_size=5):\n",
        "        \"\"\"\n",
        "        Batch generator\n",
        "\n",
        "        :param batch_size: Elements in batch\n",
        "        :return: Next batch\n",
        "        \"\"\"\n",
        "\n",
        "        # Dataset is big enought\n",
        "        # Let's skip last nonfull batch if exist\n",
        "        count_batches = (len(self.tokens) - 2 * self.window_size) // batch_size\n",
        "        print(f'count batches: {count_batches}')\n",
        "\n",
        "        for batch_id in range(count_batches):\n",
        "            batch_x = []\n",
        "            batch_label = []\n",
        "            for step_id in range(batch_size):\n",
        "                pos_word_central = step_id + self.window_size + batch_id * batch_size\n",
        "                x = self.tokens_ind[pos_word_central]\n",
        "                batch_x.append(x)\n",
        "                label_left = self.tokens_ind[pos_word_central - self.window_size : pos_word_central]\n",
        "                label_right = self.tokens_ind[pos_word_central + 1 : pos_word_central + 1 + self.window_size]\n",
        "                label = label_left + label_right\n",
        "                batch_label.append(label)\n",
        "\n",
        "            batch_x = torch.tensor(batch_x)\n",
        "            batch_label = torch.tensor(batch_label)\n",
        "            if self.mode == 'cbow':\n",
        "                batch_x, batch_label = batch_label, batch_x\n",
        "            yield batch_x, batch_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKeQtXG7CIDU",
        "colab_type": "text"
      },
      "source": [
        "# NegativeSamplingBatcher\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh-TcFRUCHgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NegativeSamplingBatcher(Batcher):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.counter_words = {}\n",
        "        # self.probabilities = np.array\n",
        "\n",
        "    #ToDo Update parent class method\n",
        "    def _build_vocabulary(self):\n",
        "        counter_words = Counter(self.tokens)\n",
        "        self.counter_words = counter_words\n",
        "        self.vocabulary = {word for word, counts in counter_words.items() if counts >= self.limit}\n",
        "        most_common = counter_words.most_common(200)\n",
        "        self.most_common = list(zip(*most_common))[0]\n",
        "\n",
        "    def _preprocess(self):\n",
        "        super()._preprocess()\n",
        "        self._count_distribution()\n",
        "\n",
        "    def _count_distribution(self):\n",
        "        frequencies = np.array([self.counter_words[word] for word in self.index2word])\n",
        "        frequencies = frequencies ** (3/4)\n",
        "        self.probabilities = frequencies / np.sum(frequencies)\n",
        "\n",
        "    def get_negative_samples(self, samples=5):\n",
        "        return torch.from_numpy(np.random.choice(len(self.probabilities), size=samples, p=self.probabilities))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "76bRKGPvYzBX",
        "colab_type": "text"
      },
      "source": [
        "## Settings for batcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YDORu2-kYzBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 3\n",
        "batch_size = 256\n",
        "limit4dictonary = 10\n",
        "mode='cbow'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "59ZXKfp7YzBi",
        "colab_type": "text"
      },
      "source": [
        "## Batcher initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nmeKLT-fYzBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batcher = NegativeSamplingBatcher.from_file(path='./text8', limit=limit4dictonary, window_size=window_size, mode=mode)\n",
        "batch_generator = batcher.batch_generator(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnXsVN2YZf-Y",
        "colab_type": "text"
      },
      "source": [
        "## NN to train CBOW_NS word2wec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pBiPmXbXYzBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW_NS(nn.Module):\n",
        "    def __init__(self, size_vocab, size_emb):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(size_vocab, size_emb)\n",
        "        self.out = nn.Linear(size_emb, size_vocab, bias=False)\n",
        "        self.sm = nn.LogSoftmax(dim=0)\n",
        "\n",
        "    def forward(self, batch_x):\n",
        "        batch_x = self.emb(batch_x)\n",
        "        out = self.sm(self.out(batch_x)) \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PQsEDnvrYzB5",
        "colab_type": "text"
      },
      "source": [
        "## Model and training settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ch8hYFjtYzB7",
        "colab_type": "code",
        "outputId": "bc8e0485-3bf4-4396-e188-4a6599367d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "size_vocab = len(batcher.vocabulary)\n",
        "size_emb = 300\n",
        "learning_rate = 0.002\n",
        "epochs = 3\n",
        "print_every = 300\n",
        "\n",
        "USE_GPU = True\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tw62a_YBYzCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CBOW_NS(size_vocab=size_vocab, size_emb=size_emb)\n",
        "model = model.to(device=device)\n",
        "\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "fnc_loss = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nb-YnOuHYzCM",
        "colab_type": "text"
      },
      "source": [
        "## Training word vectors\n",
        "- No early stopping used\n",
        "- No learning rate decreasing used\n",
        "- One hrs to deadline... No time to realize matrix version.... Khm! Let's use two losses!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "Mb2clEjmYzCR",
        "colab_type": "code",
        "outputId": "f67c7eee-c975-4b77-e14f-584035b0bea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "#Some magic to mention somewhere, how long it took to train\n",
        "\n",
        "%%time\n",
        "\n",
        "batches2plot = []\n",
        "losses2plot = []\n",
        "j = 0\n",
        "try:\n",
        "    for epoch in range(epochs):\n",
        "        i = 0\n",
        "        loss2print = 0\n",
        "        for batch_idx, (batch_x, batch_labels) in enumerate(batcher.batch_generator(batch_size)):\n",
        "            batch_x = batch_x.to(device=device, dtype=torch.long)\n",
        "            batch_labels = batch_labels.to(device=device, dtype=torch.long)\n",
        "\n",
        "            batch_x_neg = batcher.get_negative_samples(batch_size)\n",
        "            batch_x_neg = batch_x_neg.to(device=device, dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            labels_pred = model(batch_x)\n",
        "\n",
        "\n",
        "            labels_pred_neg = model(batch_x_neg)\n",
        "            loss_neg = -fnc_loss(labels_pred_neg, batch_labels)\n",
        "            loss_neg /= batch_size\n",
        "            loss_neg.backward()\n",
        "\n",
        "\n",
        "            batch_labels = batch_labels.reshape(-1, 1).expand(-1, batch_x.size()[1]).flatten()\n",
        "            labels_pred = labels_pred.reshape(-1, labels_pred.shape[-1])\n",
        "            loss = fnc_loss(labels_pred, batch_labels)\n",
        "\n",
        "            \n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss2print += float(loss)\n",
        "            i+=1\n",
        "\n",
        "            if batch_idx % print_every == 0:\n",
        "                loss2print = loss2print / i\n",
        "                batches2plot.append(j * print_every)\n",
        "                j += 1\n",
        "                losses2plot.append(loss2print)\n",
        "\n",
        "                print(f'Epoch: {epoch} Iteration: {batch_idx}, loss = {loss2print}')\n",
        "                i = 0\n",
        "                loss2print = 0\n",
        "except KeyboardInterrupt:\n",
        "    print('Training Stopped')"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count batches: 66426\n",
            "Epoch: 0 Iteration: 0, loss = 5.716185092926025\n",
            "Epoch: 0 Iteration: 300, loss = 5.591639102300008\n",
            "Epoch: 0 Iteration: 600, loss = 5.588959948221842\n",
            "Epoch: 0 Iteration: 900, loss = 5.579988498687744\n",
            "Epoch: 0 Iteration: 1200, loss = 5.580705803235372\n",
            "Epoch: 0 Iteration: 1500, loss = 5.597194746335347\n",
            "Epoch: 0 Iteration: 1800, loss = 5.578658475875854\n",
            "Epoch: 0 Iteration: 2100, loss = 5.5929874308904015\n",
            "Epoch: 0 Iteration: 2400, loss = 5.569928046862285\n",
            "Epoch: 0 Iteration: 2700, loss = 5.600867609977723\n",
            "Epoch: 0 Iteration: 3000, loss = 5.557397723197937\n",
            "Epoch: 0 Iteration: 3300, loss = 5.59805239200592\n",
            "Training Stopped\n",
            "CPU times: user 1min 23s, sys: 49.9 s, total: 2min 12s\n",
            "Wall time: 2min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Yv9rpO6vYzCX",
        "colab_type": "text"
      },
      "source": [
        "# Plotted Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "AxS0PEmhYzCb",
        "colab_type": "code",
        "outputId": "71f6cac8-959d-49c2-f99c-b92fed947957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "sns.lineplot(batches2plot, losses2plot).set(xlabel='Batch', ylabel='Loss')"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0.5, 'Loss'), Text(0.5, 0, 'Batch')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxpiALEEYY4wJ6P8DaVDLSnAiUdMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gClTtVt8Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}