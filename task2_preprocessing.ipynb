{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Assignment 1.2: Word2vec preprocessing (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Preprocessing is not the most exciting part of NLP, but it is still one of the most important ones. Your task is to preprocess raw text (you can use your own, or [this one](http://mattmahoney.net/dc/text8.zip). For this task text preprocessing mostly consists of:\n",
    "\n",
    "1. cleaning (mostly, if your dataset is from social media or parsed from the internet)\n",
    "1. tokenization\n",
    "1. building the vocabulary and choosing its size. Use only high-frequency words, change all other words to UNK or handle it in your own manner. You can use `collections.Counter` for that.\n",
    "1. assigning each token a number (numericalization). In other words, make word2index Ð¸ index2word objects.\n",
    "1. data structuring and batching - make X and y matrices generator for word2vec (explained in more details below)\n",
    "\n",
    "**ATTN!:** If you use your own data, please, attach a download link.\n",
    "\n",
    "Your goal is to make **Batcher** class which returns two numpy tensors with word indices. It should be possible to use one for word2vec training. You can implement batcher for Skip-Gram or CBOW architecture, the picture below can be helpful to remember the difference.\n",
    "\n",
    "![text](https://raw.githubusercontent.com/deepmipt/deep-nlp-seminars/651804899d05b96fc72b9474404fab330365ca09/seminar_02/pics/architecture.png)\n",
    "\n",
    "There are several ways to do it right. Shapes could be `x_batch.shape = (batch_size, 2*window_size)`, `y_batch.shape = (batch_size,)` for CBOW or `(batch_size,)`, `(batch_size, 2*window_size)` for Skip-Gram. You should **not** do negative sampling here.\n",
    "\n",
    "They should be adequately parametrized: CBOW(window_size, ...), SkipGram(window_size, ...). You should implement only one batcher in this task; and it's up to you which one to chose.\n",
    "\n",
    "Useful links:\n",
    "1. [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "You can write the code in this notebook, or in a separate file. It can be reused for the next task. The result of your work should represent that your batch has a proper structure (right shapes) and content (words should be from one context, not some random indices). To show that, translate indices back to words and print them to show something like this:\n",
    "\n",
    "```\n",
    "text = ['first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including']\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "# CBOW:\n",
    "indices_to_words(x_batch) = \\\n",
    "        [['first', 'used', 'early', 'working'],\n",
    "        ['used', 'against', 'working', 'class'],\n",
    "        ['against', 'early', 'class', 'radicals'],\n",
    "        ['early', 'working', 'radicals', 'including']]\n",
    "\n",
    "indices_to_words(labels_batch) = ['against', 'early', 'working', 'class']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher():\n",
    "    def __init__(self, text, limit=5, window_size=2, mode='sg'):\n",
    "        \"\"\"\n",
    "        Batcher for Skip-Gram or CBOW\n",
    "\n",
    "        :param text: String without newline symbols\n",
    "        :param limit: Don't put words with less amount into the dictonary\n",
    "        :param window_size: Window size ^)\n",
    "        :param mode: cbow or sg\n",
    "        \"\"\"\n",
    "\n",
    "        self.limit = limit\n",
    "        self.window_size = window_size\n",
    "        self.mode = mode\n",
    "\n",
    "        self.text = text\n",
    "\n",
    "        self.UNK = 'UNK'\n",
    "\n",
    "        self.tokens = []\n",
    "        self.tokens_ind = []\n",
    "\n",
    "\n",
    "        self.vocabulary = set()\n",
    "        self.word2index = dict()\n",
    "        self.index2word = []\n",
    "\n",
    "        self._preprocess()\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path, limit=5, window_size=2, mode='sg'):\n",
    "        \"\"\"\n",
    "        Init Batcher from file\n",
    "\n",
    "        :param path: Path to text file\n",
    "        :param limit: Don't put words with less amount into the dictonary\n",
    "        :param window_size: Window size ^)\n",
    "        :param mode: cbow or sg\n",
    "        :return: Batcher object\n",
    "        \"\"\"\n",
    "        n = -1\n",
    "        with open(path) as f:\n",
    "            text = f.read(n)\n",
    "\n",
    "        return cls(text, limit, window_size, mode)\n",
    "\n",
    "    def _clean(self):\n",
    "        # Everething expect [a-z ] already killed in our dataset\n",
    "        pass\n",
    "\n",
    "    def _tokenize(self):\n",
    "        # Stupid tokenizer for our dataset\n",
    "        self.tokens = self.text.split()\n",
    "\n",
    "    def _build_vocabulary(self):\n",
    "        counter_words = Counter(self.tokens)\n",
    "        self.vocabulary = {word for word, counts in counter_words.items() if counts >= self.limit}\n",
    "\n",
    "    def _numericalize(self):\n",
    "        self.index2word = [self.UNK] + list(self.vocabulary)\n",
    "        self.word2index = dict(zip(self.index2word, range(len(self.index2word))))\n",
    "        self.tokens_ind = [self.word2index.get(word, 0) for word in self.tokens]\n",
    "\n",
    "    def _preprocess(self):\n",
    "        self._clean()\n",
    "        self._tokenize()\n",
    "        self._build_vocabulary()\n",
    "        self._numericalize()\n",
    "\n",
    "    def indices2words(self, indices):\n",
    "        shape = indices.shape\n",
    "        result = np.array([self.index2word[idx] for idx in indices.flatten()])\n",
    "        return result.reshape(shape)\n",
    "\n",
    "    def batch_generator(self, batch_size=5):\n",
    "        \"\"\"\n",
    "        Batch generator\n",
    "\n",
    "        :param batch_size: Elements in batch\n",
    "        :return: Next batch\n",
    "        \"\"\"\n",
    "\n",
    "        # Dataset is big enought\n",
    "        # Let's skip last nonfull batch if exist\n",
    "        count_batches = (len(self.tokens) - 2 * self.window_size) // batch_size\n",
    "\n",
    "        for batch_id in range(count_batches):\n",
    "            batch_x = []\n",
    "            batch_label = []\n",
    "            for step_id in range(batch_size):\n",
    "                pos_word_central = step_id + self.window_size + batch_id * batch_size\n",
    "                x = self.tokens_ind[pos_word_central]\n",
    "                batch_x.append(x)\n",
    "                label_left = self.tokens_ind[pos_word_central - self.window_size : pos_word_central]\n",
    "                label_right = self.tokens_ind[pos_word_central + 1 : pos_word_central + 1 + self.window_size]\n",
    "                label = label_left + label_right\n",
    "                batch_label.append(label)\n",
    "\n",
    "            batch_x = np.array(batch_x)\n",
    "            batch_label = np.array(batch_label)\n",
    "            if self.mode == 'cbow':\n",
    "                batch_x, batch_label = batch_label, batch_x\n",
    "            yield batch_x, batch_label\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using with assigment file (SkipGram mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "********************\nwindow_size: 5, batch_size: 12, limit4dictonary: 4, mode: sg\n********************\n********************\nbatch_x:  [ 3595 54246 71485 22078 35302  8800  8424  1850 72335 26442]\nbatch_label: \n [[35487 50331 58537 43350 46266 54246 71485 22078 35302  8800]\n [50331 58537 43350 46266  3595 71485 22078 35302  8800  8424]\n [58537 43350 46266  3595 54246 22078 35302  8800  8424  1850]\n [43350 46266  3595 54246 71485 35302  8800  8424  1850 72335]\n [46266  3595 54246 71485 22078  8800  8424  1850 72335 26442]\n [ 3595 54246 71485 22078 35302  8424  1850 72335 26442 20044]\n [54246 71485 22078 35302  8800  1850 72335 26442 20044 49601]\n [71485 22078 35302  8800  8424 72335 26442 20044 49601  3595]\n [22078 35302  8800  8424  1850 26442 20044 49601  3595 20044]\n [35302  8800  8424  1850 72335 20044 49601  3595 20044 13365]]\n********************\n********************\nbatch_x:  [20044 49601  3595 20044 13365 70902 53863 20044 79434 37341]\nbatch_label: \n [[ 8800  8424  1850 72335 26442 49601  3595 20044 13365 70902]\n [ 8424  1850 72335 26442 20044  3595 20044 13365 70902 53863]\n [ 1850 72335 26442 20044 49601 20044 13365 70902 53863 20044]\n [72335 26442 20044 49601  3595 13365 70902 53863 20044 79434]\n [26442 20044 49601  3595 20044 70902 53863 20044 79434 37341]\n [20044 49601  3595 20044 13365 53863 20044 79434 37341  3595]\n [49601  3595 20044 13365 70902 20044 79434 37341  3595 20044]\n [ 3595 20044 13365 70902 53863 79434 37341  3595 20044 78339]\n [20044 13365 70902 53863 20044 37341  3595 20044 78339 70902]\n [13365 70902 53863 20044 79434  3595 20044 78339 70902 59022]]\n********************\n********************\nbatch_x:  [ 3595 20044 78339 70902 59022 20044 46266 15417 11938 22078]\nbatch_label: \n [[70902 53863 20044 79434 37341 20044 78339 70902 59022 20044]\n [53863 20044 79434 37341  3595 78339 70902 59022 20044 46266]\n [20044 79434 37341  3595 20044 70902 59022 20044 46266 15417]\n [79434 37341  3595 20044 78339 59022 20044 46266 15417 11938]\n [37341  3595 20044 78339 70902 20044 46266 15417 11938 22078]\n [ 3595 20044 78339 70902 59022 46266 15417 11938 22078 48293]\n [20044 78339 70902 59022 20044 15417 11938 22078 48293 43350]\n [78339 70902 59022 20044 46266 11938 22078 48293 43350 50451]\n [70902 59022 20044 46266 15417 22078 48293 43350 50451 75917]\n [59022 20044 46266 15417 11938 48293 43350 50451 75917 16291]]\n********************\n********************\nbatch_x shape: (10,), batch_label shape: (10, 10)\n"
    }
   ],
   "source": [
    "window_size = 5\n",
    "batch_size = 12\n",
    "limit4dictonary = 4\n",
    "mode='sg'\n",
    "# mode='cbow'\n",
    "\n",
    "print('*' * 20)\n",
    "print(f'window_size: {window_size}, batch_size: {batch_size}, limit4dictonary: {limit4dictonary}, mode: {mode}')\n",
    "print('*' * 20)\n",
    "\n",
    "batcher = Batcher.from_file(path='./text8', limit=limit4dictonary, window_size=window_size)\n",
    "batch_generator = batcher.batch_generator(10)\n",
    "\n",
    "i = 0\n",
    "for batch_x, batch_label in batch_generator:\n",
    "    print('*' * 20)\n",
    "    print('batch_x: ', batch_x)\n",
    "    print(\"batch_label: \\n\", batch_label)\n",
    "    print('*' * 20)\n",
    "\n",
    "    i+=1\n",
    "    if i >= 3:\n",
    "        break\n",
    "print('*' * 20)\n",
    "print(f'batch_x shape: {batch_x.shape}, batch_label shape: {batch_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using with assigment text sample (CBOW mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "********************\nbatch_x: \n [[7 8 1 5]\n [8 2 5 3]\n [2 1 3 4]\n [1 5 4 6]]\nbatch_label: \n [2 1 5 3]\n********************\ntext: first used against early working class radicals including\nbatch_x: \n [['first' 'used' 'early' 'working']\n ['used' 'against' 'working' 'class']\n ['against' 'early' 'class' 'radicals']\n ['early' 'working' 'radicals' 'including']]\n********************\nbatch_label: \n ['against' 'early' 'working' 'class']\n"
    }
   ],
   "source": [
    "test_text = 'first used against early working class radicals including'\n",
    "test_batcher = Batcher(test_text, limit=1, window_size=2, mode='cbow')\n",
    "test_batch_generator = test_batcher.batch_generator(4)\n",
    "for batch_x, batch_label in test_batch_generator:\n",
    "    print('*' * 20)\n",
    "    print(\"batch_x: \\n\", batch_x)\n",
    "    print(\"batch_label: \\n\", batch_label)\n",
    "    print('*' * 20)\n",
    "\n",
    "print(f'text: {test_text}')\n",
    "print(\"batch_x: \\n\", test_batcher.indices2words(batch_x))\n",
    "print('*' * 20)\n",
    "print(\"batch_label: \\n\", test_batcher.indices2words(batch_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}